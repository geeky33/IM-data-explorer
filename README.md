## ðŸ“¦ Block 1: Datumaro Integration Layer

- [x] Understand Datumaro API & plugin system
- [x] Clone and install Datumaro in editable mode
- [ ] Load and test COCO dataset using `datumaro.Dataset.import()`
- [ ] Load and test YOLO dataset using `datumaro.Dataset.import()`
- [ ] Normalize labels and metadata across formats
- [ ] Design simple plugin interface for embedding storage (hook to HDF5/mmap)
- [ ] Write utility to convert Datumaro items â†’ embedding input format
- [ ] Test dataset loading on 2+ formats
- [ ] Validate metadata integrity (ID, label, caption consistency)



## ðŸ§  Block 2: Embedding Module

- [x] Install `transformers`, `optimum`, and `openvino`
- [x] Export `openai/clip-vit-base-patch32` to OpenVINO IR (image encoder)
- [x] Export `openai/clip-vit-base-patch32` to OpenVINO IR (text encoder)
- [X] Implement `extractor.py` for image embeddings using OpenVINO
- [X] Implement `extractor.py` for text embeddings using OpenVINO
- [X] Write `storage.py` for storing embeddings in HDF5 with metadata
- [X] Create CLI/utility for batch embedding generation
- [ ] Write unit tests for extractor on a sample dataset (10â€“100 items)
- [ ] Test large-scale extraction (1000+ samples)
- [ ] Optimize performance (batching, inference caching)
- [ ] Add error handling: missing file, inference fail, etc.

## ðŸ”„ Integration Tasks (Optional for Week 3)

- [ ] Connect Datumaro loader output to embedding extractor
- [ ] Store embeddings + metadata using unified format
- [ ] Build debug CLI to extract + store + print shape and ID

# ðŸ§  Interactive Multimodal Data Explorer (GSoC 2025 - Midterm Report)

## ðŸ“Œ Project Overview

This GSoC project aims to build an **Interactive Multimodal Data Explorer** that uses OpenVINO-accelerated CLIP models to extract, store, and visualize embeddings from datasets in multiple formats like COCO, YOLO, etc., with integration to Datumaro and a lightweight Streamlit frontend.

---

## âœ… Progress Summary (till Midterm)

### ðŸ”¹ Completed

- **Environment Setup**
  - Created isolated Python virtual environment
  - Installed all core libraries: `openvino`, `optimum`, `transformers`, `datumaro`, etc.

- **Model Export**
  - Exported `openai/clip-vit-base-patch32` to OpenVINO IR using Docker
  - Image encoder and text encoder exported successfully to `clip_ov/`

- **Embedding Pipeline**
  - Implemented `extractor.py` for both image and text encoding using OpenVINO runtime
  - Implemented `storage.py` to save embeddings in HDF5 format with metadata
  - Created `test_embed.py` for local testing of 2â€“3 image files (cat, dog, car)

- **File Structure**
  - Organized `src/core/embeddings/`, `scripts/`, and testing files clearly
  - Sample images and test setup ready

---

### ðŸŸ¡ In Progress / Partial

- **CLI Tool**
  - `test_embed.py` simulates batch embedding generation, needs full CLI wrapper

- **Streamlit App**
  - Basic skeleton started, will connect once embedding generation is stabilized

---

### âŒ Not Achieved Yet

- **Datumaro Dataset Integration**
  - Haven't yet loaded or converted datasets like COCO/YOLO using `datumaro.Dataset.import(...)`
  - Normalization and format bridging code is pending

- **Unit Tests**
  - No automated test suite written yet for extractor/storage modules

- **Large-scale Dataset Embedding**
  - No benchmark dataset (>1000 items) tested yet due to missing Datumaro loader

- **Full CLI + Error Handling**
  - Embedding pipeline doesn't yet handle failure cases or CLI flags

---

## ðŸ§± Folder Structure



1. Validation: â€œWhat acceptance test should we use for the embedding extractor?â€
2. Performance targets: â€œAny latency/throughput numbers we must hit?â€
3. Data: â€œWhich benchmark dataset would you prefer I cache first?â€
4. Integration: â€œConfirm the API shape for Datumaro ingestâ€”Python class or CLI?â€
5. UI timeline: â€œIs a bare-bones Streamlit mock-up by Week 4 acceptable?â€

| **Roadblock** | **What actually happened** | **Why it tripped you up** | **Quick fix / lesson learned** |
|---|---|---|---|
| `optimum-cli export openvino` kept failing | The command threw shape-mismatch and â€œunsupported opâ€ errors when converting the CLIP model to OpenVINO IR. | The default ONNX version bundled with **optimum** was older than what OpenVINO expects. | Pin compatible versions (`pip install onnx==1.16.0 optimum==1.18.1`) *before* export. Record the combo for future setups. |
| CUDA vs. OpenVINO driver clash | OpenVINOâ€™s GPU plugin grabbed the Intel iGPU instead of your RTX 3060, then silently fell back to CPU. | Mixed-GPU environments confuse OpenVINO unless the device is forced. | Specify the target: `compile_model(..., device_name="GPU.1")` or stick to CPU until later benchmarks. |
| Missing system dependencies | CMake and protobuf headers werenâ€™t on `PATH`, so the model-optimizer script bailed. | OpenVINO calls these tools under the hood. | `sudo apt install cmake protobuf-compiler libprotobuf-dev` and note this in the README so no one repeats it. |
| Slow hostel Wi-Fi for model download | The 400 MB CLIP model from Hugging Face timed out twice. | Unstable network cost several hours. | Cache the model to a USB drive and use `--local-files-only` on retries; consider `aria2c` or campus LAN. |
| No visible progress log for mentors | Work stayed on your laptop; nothing pushed for four days. | Mentors couldnâ€™t step in because they didnâ€™t know you were blocked. | Open an â€œExport Blockerâ€ issue once stuck > 2 hrs and push WIP branches nightly. |

*Take-away: most blockers were version mismatches or silent fallbacksâ€”catch them early and surface them quickly to save whole days later.*

